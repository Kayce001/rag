#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´RAGæ£€ç´¢é‡æ’åºç³»ç»Ÿ
ç»“åˆEmbeddingæ£€ç´¢ + Rerankeré‡æ’åº
é€‚åˆåˆå­¦è€…å­¦ä¹ çš„ç®€åŒ–ç‰ˆæœ¬
"""

import math
import warnings
from typing import List, Tuple, Dict
import torch

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore")

try:
    from transformers import AutoTokenizer
    from vllm import LLM, SamplingParams
    from vllm.inputs.data import TokensPrompt
    print("âœ“ æˆåŠŸå¯¼å…¥æ‰€æœ‰å¿…éœ€åº“")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·å®‰è£…: pip install vllm transformers torch")
    exit(1)


class SimpleRAGSystem:
    """ç®€åŒ–ç‰ˆRAGæ£€ç´¢é‡æ’åºç³»ç»Ÿ"""
    
    def __init__(self, embed_model_path: str, rerank_model_path: str):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            embed_model_path: åµŒå…¥æ¨¡å‹è·¯å¾„
            rerank_model_path: é‡æ’åºæ¨¡å‹è·¯å¾„
        """
        print("ğŸš€ åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        
        # 1. åŠ è½½åµŒå…¥æ¨¡å‹
        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {embed_model_path}")
        self.embed_model = LLM(model=embed_model_path, task="embed")
        
        # 2. åŠ è½½é‡æ’åºæ¨¡å‹
        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½é‡æ’åºæ¨¡å‹: {rerank_model_path}")
        self.rerank_tokenizer = AutoTokenizer.from_pretrained(
            rerank_model_path, 
            trust_remote_code=True
        )
        self.rerank_model = LLM(
            model=rerank_model_path,
            max_model_len=8192,
            trust_remote_code=True,
            gpu_memory_utilization=0.8
        )
        
        # 3. è®¾ç½®é‡æ’åºå‚æ•°
        self._setup_reranker()
        
        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _setup_reranker(self):
        """è®¾ç½®é‡æ’åºå™¨å‚æ•°"""
        self.suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        self.suffix_tokens = self.rerank_tokenizer.encode(self.suffix, add_special_tokens=False)
        
        # è·å–yes/noå¯¹åº”çš„token ID
        self.yes_token = self.rerank_tokenizer("yes", add_special_tokens=False).input_ids[0]
        self.no_token = self.rerank_tokenizer("no", add_special_tokens=False).input_ids[0]
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        self.sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=1,
            logprobs=20,
            allowed_token_ids=[self.yes_token, self.no_token]
        )
    
    def get_detailed_instruct(self, task_description: str, query: str) -> str:
        """åˆ›å»ºè¯¦ç»†æŒ‡ä»¤"""
        return f'Instruct: {task_description}\nQuery:{query}'
    
    def embed_texts(self, texts: List[str]) -> torch.Tensor:
        """
        å¯¹æ–‡æœ¬è¿›è¡ŒåµŒå…¥ç¼–ç 
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            åµŒå…¥å‘é‡å¼ é‡
        """
        outputs = self.embed_model.embed(texts)
        embeddings = torch.tensor([o.outputs.embedding for o in outputs])
        return embeddings
    
    def retrieve_by_embedding(self, query: str, documents: List[str], 
                            task: str = "Given a web search query, retrieve relevant passages that answer the query",
                            top_k: int = 10) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨åµŒå…¥å‘é‡è¿›è¡Œåˆæ­¥æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            task: ä»»åŠ¡æè¿°
            top_k: è¿”å›top-kä¸ªç»“æœ
            
        Returns:
            æŒ‰ç›¸ä¼¼åº¦æ’åºçš„(æ–‡æ¡£, ç›¸ä¼¼åº¦)åˆ—è¡¨
        """
        print(f"ğŸ” ä½¿ç”¨åµŒå…¥å‘é‡æ£€ç´¢ top-{top_k} æ–‡æ¡£...")
        
        # å‡†å¤‡è¾“å…¥æ–‡æœ¬
        detailed_query = self.get_detailed_instruct(task, query)
        input_texts = [detailed_query] + documents
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = self.embed_texts(input_texts)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        query_embedding = embeddings[0:1]  # æŸ¥è¯¢å‘é‡
        doc_embeddings = embeddings[1:]    # æ–‡æ¡£å‘é‡
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity_scores = (query_embedding @ doc_embeddings.T).squeeze()
        
        # åˆ›å»ºç»“æœåˆ—è¡¨
        results = []
        for i, (doc, score) in enumerate(zip(documents, similarity_scores)):
            results.append((doc, float(score)))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top-k
        results.sort(key=lambda x: x[1], reverse=True)
        top_results = results[:top_k]
        
        print(f"âœ… åµŒå…¥æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(top_results)} ä¸ªç»“æœ")
        return top_results
    
    def create_rerank_prompt(self, instruction: str, query: str, document: str) -> List[dict]:
        """åˆ›å»ºé‡æ’åºæç¤ºè¯"""
        return [
            {
                "role": "system", 
                "content": "Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\"."
            },
            {
                "role": "user", 
                "content": f"<Instruct>: {instruction}\n\n<Query>: {query}\n\n<Document>: {document}"
            }
        ]
    
    def rerank_score(self, query: str, document: str, 
                    instruction: str = "Given a web search query, retrieve relevant passages that answer the query") -> float:
        """
        è®¡ç®—é‡æ’åºè¯„åˆ†
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            document: æ–‡æ¡£æ–‡æœ¬
            instruction: ä»»åŠ¡æŒ‡ä»¤
            
        Returns:
            é‡æ’åºè¯„åˆ† (0-1ä¹‹é—´)
        """
        # åˆ›å»ºæç¤ºè¯
        messages = [self.create_rerank_prompt(instruction, query, document)]
        
        # è½¬æ¢ä¸ºtokens
        tokenized = self.rerank_tokenizer.apply_chat_template(
            messages, 
            tokenize=True, 
            add_generation_prompt=False
        )[0]
        
        # æ·»åŠ åç¼€å¹¶åˆ›å»ºè¾“å…¥
        full_tokens = tokenized + self.suffix_tokens
        token_prompt = TokensPrompt(prompt_token_ids=full_tokens)
        
        # æ¨¡å‹æ¨ç†
        outputs = self.rerank_model.generate([token_prompt], self.sampling_params)
        
        # è·å–logprobs
        logprobs = outputs[0].outputs[0].logprobs[-1]
        
        # è®¡ç®—yeså’Œnoçš„æ¦‚ç‡
        yes_logprob = logprobs.get(self.yes_token, type('obj', (object,), {'logprob': -10})()).logprob
        no_logprob = logprobs.get(self.no_token, type('obj', (object,), {'logprob': -10})()).logprob
        
        yes_prob = math.exp(yes_logprob)
        no_prob = math.exp(no_logprob)
        
        # å½’ä¸€åŒ–å¾—åˆ°æœ€ç»ˆè¯„åˆ†
        total_prob = yes_prob + no_prob
        score = yes_prob / total_prob if total_prob > 0 else 0.0
        
        return score
    
    def rerank_documents(self, query: str, documents: List[Tuple[str, float]], 
                        instruction: str = "Given a web search query, retrieve relevant passages that answer the query") -> List[Tuple[str, float, float]]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: (æ–‡æ¡£, åµŒå…¥ç›¸ä¼¼åº¦)åˆ—è¡¨
            instruction: ä»»åŠ¡æŒ‡ä»¤
            
        Returns:
            (æ–‡æ¡£, åµŒå…¥ç›¸ä¼¼åº¦, é‡æ’åºè¯„åˆ†)åˆ—è¡¨ï¼ŒæŒ‰é‡æ’åºè¯„åˆ†æ’åº
        """
        print(f"ğŸ¯ æ­£åœ¨å¯¹ {len(documents)} ä¸ªæ–‡æ¡£è¿›è¡Œé‡æ’åº...")
        
        results = []
        for i, (doc, embed_score) in enumerate(documents):
            rerank_score = self.rerank_score(query, doc, instruction)
            results.append((doc, embed_score, rerank_score))
            print(f"  æ–‡æ¡£ {i+1}/{len(documents)}: åµŒå…¥={embed_score:.3f}, é‡æ’åº={rerank_score:.3f}")
        
        # æŒ‰é‡æ’åºè¯„åˆ†æ’åº
        results.sort(key=lambda x: x[2], reverse=True)
        
        print("âœ… é‡æ’åºå®Œæˆ")
        return results
    
    def search(self, query: str, documents: List[str], 
              task: str = "Given a web search query, retrieve relevant passages that answer the query",
              embed_top_k: int = 10, final_top_k: int = 5) -> List[Dict]:
        """
        å®Œæ•´çš„RAGæ£€ç´¢æµç¨‹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åº“
            task: ä»»åŠ¡æè¿°
            embed_top_k: åµŒå…¥æ£€ç´¢è¿”å›çš„æ–‡æ¡£æ•°é‡
            final_top_k: æœ€ç»ˆè¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            æœ€ç»ˆæ’åºç»“æœåˆ—è¡¨
        """
        print(f"\nğŸ” å¼€å§‹RAGæ£€ç´¢: '{query}'")
        print(f"ğŸ“š æ–‡æ¡£åº“å¤§å°: {len(documents)}")
        print("="*60)
        
        # ç¬¬ä¸€æ­¥ï¼šåµŒå…¥å‘é‡æ£€ç´¢
        embed_results = self.retrieve_by_embedding(query, documents, task, embed_top_k)
        
        # ç¬¬äºŒæ­¥ï¼šé‡æ’åº
        rerank_results = self.rerank_documents(query, embed_results, task)
        
        # ç¬¬ä¸‰æ­¥ï¼šè¿”å›æœ€ç»ˆç»“æœ
        final_results = []
        for i, (doc, embed_score, rerank_score) in enumerate(rerank_results[:final_top_k]):
            final_results.append({
                'rank': i + 1,
                'document': doc,
                'embed_score': embed_score,
                'rerank_score': rerank_score,
                'relevance': 'ğŸ”¥ é«˜åº¦ç›¸å…³' if rerank_score > 0.7 else 'ğŸ“„ ä¸­ç­‰ç›¸å…³' if rerank_score > 0.4 else 'â“ ä½ç›¸å…³'
            })
        
        return final_results


def demo():
    """æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ å®Œæ•´RAGæ£€ç´¢é‡æ’åºç³»ç»Ÿæ¼”ç¤º")
    print("="*60)
    
    # è¯·ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
    EMBED_MODEL_PATH = "/mnt/e/qwenrag/embed"      # åµŒå…¥æ¨¡å‹è·¯å¾„
    RERANK_MODEL_PATH = "/mnt/e/qwenrag/rerank"    # é‡æ’åºæ¨¡å‹è·¯å¾„
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_system = SimpleRAGSystem(EMBED_MODEL_PATH, RERANK_MODEL_PATH)
    
    # æµ‹è¯•æ•°æ®
    query = "What is the capital of China?"
    documents = [
        "The capital of China is Beijing. It is located in northern China and serves as the political center.",
        "China is a large country in East Asia with over 1.4 billion people and rich cultural heritage.",
        "Beijing is the capital and political center of China, known for its historical landmarks.",
        "Shanghai is the most populous city in China and a major financial hub.",
        "The Great Wall of China is a famous tourist attraction that stretches across northern China.",
        "Guangzhou is a major city in southern China known for its trade and commerce.",
        "Xi'an is an ancient city in China, famous for the Terracotta Warriors.",
        "Shenzhen is a modern city in China, known for its technology industry.",
        "Chengdu is the capital of Sichuan province, famous for pandas and spicy food.",
        "Hangzhou is known for its beautiful West Lake and as the headquarters of Alibaba."
    ]
    
    # æ‰§è¡Œå®Œæ•´RAGæ£€ç´¢
    results = rag_system.search(
        query=query,
        documents=documents,
        embed_top_k=6,  # åµŒå…¥æ£€ç´¢è¿”å›å‰6ä¸ª
        final_top_k=3   # æœ€ç»ˆè¿”å›å‰3ä¸ª
    )
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ† æœ€ç»ˆæ£€ç´¢ç»“æœ:")
    print("="*60)
    for result in results:
        print(f"\n{result['rank']}. {result['relevance']}")
        print(f"   åµŒå…¥ç›¸ä¼¼åº¦: {result['embed_score']:.3f}")
        print(f"   é‡æ’åºè¯„åˆ†: {result['rerank_score']:.3f}")
        print(f"   æ–‡æ¡£å†…å®¹: {result['document']}")


if __name__ == "__main__":
    demo()
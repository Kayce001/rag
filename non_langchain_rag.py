#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½»é‡çº§RAGæ£€ç´¢é‡æ’åºç³»ç»Ÿ
å»é™¤LangChainä¾èµ–ï¼Œä½¿ç”¨çº¯Pythonå®ç°
é€‚ç”¨äºå·²é¢„å¤„ç†çš„TXTæ–‡æ¡£
"""
#nice
import os
import pickle
import json
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
import warnings
import torch
import numpy as np
from pathlib import Path
import hashlib

# æœ¬åœ°å‘é‡å­˜å‚¨
import faiss

# VLLMå’ŒTransformers
try:
    from vllm import LLM, SamplingParams
    from vllm.inputs.data import TokensPrompt
    from transformers import AutoTokenizer
    print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰å¿…éœ€åº“")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·å®‰è£…: pip install faiss-cpu vllm transformers torch")
    exit(1)

warnings.filterwarnings("ignore")


@dataclass
class Document:
    """è½»é‡çº§æ–‡æ¡£ç±»ï¼Œæ›¿ä»£LangChainçš„Document"""
    page_content: str
    metadata: Dict = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class RAGConfig:
    """RAGç³»ç»Ÿé…ç½®ç±»"""
    embed_model_path: str
    rerank_model_path: str
    txt_data_path: str
    vector_store_path: str = "./vector_store"
    embed_top_k: int = 10
    final_top_k: int = 5
    device: str = "cuda" if torch.cuda.is_available() else "cpu"


class TxtDocumentLoader:
    """è½»é‡çº§TXTæ–‡æ¡£åŠ è½½å™¨"""
    
    def __init__(self, data_path: str):
        self.data_path = Path(data_path)
    
    def load_documents(self) -> List[Document]:
        """åŠ è½½æ‰€æœ‰TXTæ–‡æ¡£ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰"""
        documents = []
        
        if self.data_path.is_file() and self.data_path.suffix == '.txt':
            txt_files = [self.data_path]
        elif self.data_path.is_dir():
            # é€’å½’æŸ¥æ‰¾æ‰€æœ‰å­ç›®å½•ä¸­çš„txtæ–‡ä»¶
            txt_files = list(self.data_path.rglob("*.txt"))
        else:
            raise ValueError(f"æ— æ•ˆçš„æ•°æ®è·¯å¾„: {self.data_path}")
        
        print(f"ğŸ“ å‘ç° {len(txt_files)} ä¸ªTXTæ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰")
        
        for txt_file in txt_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                
                if content:
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": str(txt_file),
                            "filename": txt_file.name,
                            "file_size": len(content),
                            "file_hash": hashlib.md5(content.encode()).hexdigest()
                        }
                    )
                    documents.append(doc)
                    print(f"  âœ… åŠ è½½: {txt_file.name} ({len(content)} å­—ç¬¦)")
                else:
                    print(f"  âš ï¸  è·³è¿‡ç©ºæ–‡ä»¶: {txt_file.name}")
                    
            except Exception as e:
                print(f"  âŒ åŠ è½½å¤±è´¥: {txt_file.name} - {str(e)}")
        
        print(f"ğŸ“š æ€»å…±åŠ è½½ {len(documents)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
        return documents


class QwenEmbeddings:
    """QwenåµŒå…¥æ¨¡å‹"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        print(f"ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹: {model_path}")
        self.model = LLM(model=model_path, task="embed")
        print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£åˆ—è¡¨"""
        if not texts:
            return []
        
        # ä¸ºæ–‡æ¡£æ·»åŠ æŒ‡ä»¤å‰ç¼€
        prefixed_texts = [
            f"Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: {text}"
            for text in texts
        ]
        
        print(f"ğŸ”„ æ­£åœ¨å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æ¡£...")
        outputs = self.model.embed(prefixed_texts)
        embeddings = [output.outputs.embedding for output in outputs]
        print(f"âœ… æ–‡æ¡£å‘é‡åŒ–å®Œæˆ")
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢æ–‡æœ¬"""
        prefixed_text = f"Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: {text}"
        outputs = self.model.embed([prefixed_text])
        return outputs[0].outputs.embedding


class QwenReranker:
    """Qwené‡æ’åºå™¨"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        print(f"ğŸ“¦ åŠ è½½é‡æ’åºæ¨¡å‹: {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = LLM(
            model=model_path,
            max_model_len=8192,
            trust_remote_code=True,
            gpu_memory_utilization=0.8
        )
        self._setup_reranker()
        print("âœ… é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _setup_reranker(self):
        """è®¾ç½®é‡æ’åºå™¨å‚æ•°"""
        self.suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)
        
        self.yes_token = self.tokenizer("yes", add_special_tokens=False).input_ids[0]
        self.no_token = self.tokenizer("no", add_special_tokens=False).input_ids[0]
        
        self.sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=1,
            logprobs=20,
            allowed_token_ids=[self.yes_token, self.no_token]
        )
    
    def _create_rerank_prompt(self, query: str, document: str, instruction: str) -> List[dict]:
        """åˆ›å»ºé‡æ’åºæç¤ºè¯"""
        return [
            {
                "role": "system",
                "content": "Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\"."
            },
            {
                "role": "user",
                "content": f"<Instruct>: {instruction}\n\n<Query>: {query}\n\n<Document>: {document}"
            }
        ]
    
    def score(self, query: str, document: str, 
              instruction: str = "Given a web search query, retrieve relevant passages that answer the query") -> float:
        """è®¡ç®—é‡æ’åºè¯„åˆ†"""
        messages = [self._create_rerank_prompt(query, document, instruction)]
        
        tokenized = self.tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=False
        )[0]
        
        full_tokens = tokenized + self.suffix_tokens
        token_prompt = TokensPrompt(prompt_token_ids=full_tokens)
        
        outputs = self.model.generate([token_prompt], self.sampling_params)
        logprobs = outputs[0].outputs[0].logprobs[-1]
        
        yes_logprob = logprobs.get(self.yes_token, type('obj', (object,), {'logprob': -10})()).logprob
        no_logprob = logprobs.get(self.no_token, type('obj', (object,), {'logprob': -10})()).logprob
        
        yes_prob = np.exp(yes_logprob)
        no_prob = np.exp(no_logprob)
        
        total_prob = yes_prob + no_prob
        score = yes_prob / total_prob if total_prob > 0 else 0.0
        
        return float(score)


class VectorStore:
    """è½»é‡çº§å‘é‡å­˜å‚¨"""
    
    def __init__(self, embeddings: QwenEmbeddings):
        self.embeddings = embeddings
        self.documents = []
        self.index = None
        self.dimension = None
    
    def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        if not documents:
            return
        
        # æå–æ–‡æ¡£å†…å®¹
        texts = [doc.page_content for doc in documents]
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = self.embeddings.embed_documents(texts)
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        if self.index is None:
            self.dimension = len(embeddings[0])
            self.index = faiss.IndexFlatIP(self.dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
            print(f"ğŸ“Š åˆ›å»ºFAISSç´¢å¼•ï¼Œç»´åº¦: {self.dimension}")
        
        # æ ‡å‡†åŒ–å‘é‡å¹¶æ·»åŠ åˆ°ç´¢å¼•
        embeddings_array = np.array(embeddings, dtype=np.float32)
        # L2æ ‡å‡†åŒ–ï¼Œä½¿å†…ç§¯ç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦
        faiss.normalize_L2(embeddings_array)
        
        self.index.add(embeddings_array)
        self.documents.extend(documents)
        
        print(f"ğŸ“Š å‘é‡ç´¢å¼•æ›´æ–°å®Œæˆï¼Œæ€»æ–‡æ¡£æ•°: {len(self.documents)}")
    
    def similarity_search_with_score(self, query: str, k: int = 10) -> List[Tuple[Document, float]]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        if self.index is None or len(self.documents) == 0:
            return []
        
        # æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = self.embeddings.embed_query(query)
        query_vector = np.array([query_embedding], dtype=np.float32)
        faiss.normalize_L2(query_vector)  # æ ‡å‡†åŒ–
        
        # æœç´¢
        k = min(k, len(self.documents))  # ç¡®ä¿kä¸è¶…è¿‡æ–‡æ¡£æ€»æ•°
        scores, indices = self.index.search(query_vector, k)
        
        # æ„å»ºç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                results.append((self.documents[idx], float(score)))
        
        return results
    
    def save_local(self, path: str):
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°æœ¬åœ°"""
        save_path = Path(path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        if self.index is not None:
            faiss.write_index(self.index, str(save_path / "index.faiss"))
        
        # ä¿å­˜æ–‡æ¡£å’Œå…ƒæ•°æ®
        docs_data = []
        for doc in self.documents:
            docs_data.append({
                'page_content': doc.page_content,
                'metadata': doc.metadata
            })
        
        with open(save_path / "documents.pkl", 'wb') as f:
            pickle.dump(docs_data, f)
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        config_info = {
            'dimension': self.dimension,
            'num_documents': len(self.documents)
        }
        with open(save_path / "config.json", 'w', encoding='utf-8') as f:
            json.dump(config_info, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_local(self, path: str) -> bool:
        """ä»æœ¬åœ°åŠ è½½å‘é‡å­˜å‚¨"""
        load_path = Path(path)
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = ['index.faiss', 'documents.pkl', 'config.json']
        for file in required_files:
            if not (load_path / file).exists():
                print(f"âŒ ç¼ºå°‘æ–‡ä»¶: {file}")
                return False
        
        try:
            # åŠ è½½é…ç½®
            with open(load_path / "config.json", 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # åŠ è½½FAISSç´¢å¼•
            self.index = faiss.read_index(str(load_path / "index.faiss"))
            self.dimension = config['dimension']
            
            # åŠ è½½æ–‡æ¡£
            with open(load_path / "documents.pkl", 'rb') as f:
                docs_data = pickle.load(f)
            
            self.documents = []
            for doc_data in docs_data:
                doc = Document(
                    page_content=doc_data['page_content'],
                    metadata=doc_data['metadata']
                )
                self.documents.append(doc)
            
            print(f"ğŸ“‚ å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸ: {len(self.documents)} ä¸ªæ–‡æ¡£ï¼Œç»´åº¦ {self.dimension}")
            return True
            
        except Exception as e:
            print(f"âŒ å‘é‡å­˜å‚¨åŠ è½½å¤±è´¥: {e}")
            return False


class RAGRetriever:
    """RAGæ£€ç´¢å™¨"""
    
    def __init__(self, vector_store: VectorStore, reranker: QwenReranker, 
                 embed_top_k: int = 10, final_top_k: int = 5):
        self.vector_store = vector_store
        self.reranker = reranker
        self.embed_top_k = embed_top_k
        self.final_top_k = final_top_k
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        print(f"\nğŸ” å¼€å§‹RAGæ£€ç´¢: '{query}'")
        print("="*60)
        
        # ç¬¬ä¸€æ­¥ï¼šå‘é‡æ£€ç´¢
        print(f"ğŸ“Š æ‰§è¡Œå‘é‡æ£€ç´¢ï¼Œè·å–top-{self.embed_top_k}æ–‡æ¡£...")
        embed_docs = self.vector_store.similarity_search_with_score(query, k=self.embed_top_k)
        
        if not embed_docs:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            return []
        
        print(f"âœ… å‘é‡æ£€ç´¢å®Œæˆï¼Œè·å¾— {len(embed_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
        
        # ç¬¬äºŒæ­¥ï¼šé‡æ’åº
        print(f"ğŸ¯ æ‰§è¡Œé‡æ’åº...")
        rerank_results = []
        
        for i, (doc, embed_score) in enumerate(embed_docs):
            rerank_score = self.reranker.score(query, doc.page_content)
            rerank_results.append((doc, embed_score, rerank_score))
            print(f"  æ–‡æ¡£ {i+1}/{len(embed_docs)}: å‘é‡={embed_score:.3f}, é‡æ’åº={rerank_score:.3f}")
        
        # æŒ‰é‡æ’åºè¯„åˆ†æ’åº
        rerank_results.sort(key=lambda x: x[2], reverse=True)
        
        # è¿”å›æœ€ç»ˆç»“æœ
        final_docs = []
        for i, (doc, embed_score, rerank_score) in enumerate(rerank_results[:self.final_top_k]):
            doc.metadata.update({
                'embed_score': embed_score,
                'rerank_score': rerank_score,
                'final_rank': i + 1
            })
            final_docs.append(doc)
        
        print(f"âœ… é‡æ’åºå®Œæˆï¼Œè¿”å›top-{len(final_docs)}ç»“æœ")
        return final_docs


class LightweightRAGSystem:
    """è½»é‡çº§RAGç³»ç»Ÿ"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.embeddings = None
        self.reranker = None
        self.vector_store = None
        self.retriever = None
        
        self._initialize_components()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        print("ğŸš€ åˆå§‹åŒ–è½»é‡çº§RAGç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = QwenEmbeddings(self.config.embed_model_path)
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        self.reranker = QwenReranker(self.config.rerank_model_path)
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vector_store = VectorStore(self.embeddings)
        
        print("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def _get_data_hash(self) -> str:
        """è·å–æ•°æ®ç›®å½•çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹æ•°æ®æ˜¯å¦å˜åŒ–"""
        data_path = Path(self.config.txt_data_path)
        
        if data_path.is_file():
            with open(data_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        
        # ç›®å½•æƒ…å†µï¼šé€’å½’è®¡ç®—æ‰€æœ‰txtæ–‡ä»¶çš„ç»¼åˆå“ˆå¸Œ
        hash_md5 = hashlib.md5()
        txt_files = sorted(data_path.rglob("*.txt"))  # é€’å½’æŸ¥æ‰¾å¹¶æ’åºç¡®ä¿ä¸€è‡´æ€§
        
        for txt_file in txt_files:
            try:
                with open(txt_file, 'rb') as f:
                    hash_md5.update(f.read())
                hash_md5.update(str(txt_file).encode())  # åŒ…å«æ–‡ä»¶å
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•è¯»å–æ–‡ä»¶ {txt_file}: {e}")
        
        return hash_md5.hexdigest()
    
    def _check_vector_store_validity(self) -> bool:
        """æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦æœ‰æ•ˆä¸”æœ€æ–°"""
        vector_path = Path(self.config.vector_store_path)
        hash_file = vector_path / "data_hash.txt"
        
        if not hash_file.exists():
            return False
        
        try:
            with open(hash_file, 'r') as f:
                stored_hash = f.read().strip()
            
            current_hash = self._get_data_hash()
            return stored_hash == current_hash
        
        except Exception:
            return False
    
    def build_vector_store(self, force_rebuild: bool = False) -> VectorStore:
        """æ„å»ºæˆ–åŠ è½½å‘é‡å­˜å‚¨"""
        vector_path = Path(self.config.vector_store_path)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        need_rebuild = force_rebuild or not self._check_vector_store_validity()
        
        if not need_rebuild and vector_path.exists():
            print("ğŸ“‚ æ£€æµ‹åˆ°æœ‰æ•ˆçš„å‘é‡å­˜å‚¨ç¼“å­˜...")
            if self.vector_store.load_local(str(vector_path)):
                print("âœ… å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸï¼Œè·³è¿‡é‡æ–°æ„å»º")
                return self.vector_store
            else:
                print("âš ï¸  å‘é‡å­˜å‚¨åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°æ„å»º")
                need_rebuild = True
        
        if need_rebuild:
            print("ğŸ—ï¸  æ„å»ºæ–°çš„å‘é‡å­˜å‚¨...")
            
            # åŠ è½½æ–‡æ¡£
            loader = TxtDocumentLoader(self.config.txt_data_path)
            documents = loader.load_documents()
            
            if not documents:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£")
            
            # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æ¡£ï¼ˆå› ä¸ºå·²ç»é¢„å¤„ç†å¥½äº†ï¼‰
            print(f"ğŸ“„ ä½¿ç”¨é¢„å¤„ç†æ–‡æ¡£: {len(documents)} ä¸ªæ–‡æ¡£")
            
            # æ„å»ºå‘é‡å­˜å‚¨
            print("ğŸ”¨ å¼€å§‹å‘é‡åŒ–æ–‡æ¡£...")
            self.vector_store.add_documents(documents)
            
            # ä¿å­˜å‘é‡å­˜å‚¨
            self.vector_store.save_local(str(vector_path))
            
            # ä¿å­˜æ•°æ®å“ˆå¸Œ
            hash_file = vector_path / "data_hash.txt"
            with open(hash_file, 'w') as f:
                f.write(self._get_data_hash())
            
            print("âœ… å‘é‡å­˜å‚¨æ„å»ºå¹¶ä¿å­˜å®Œæˆ")
        
        return self.vector_store
    
    def create_retriever(self) -> RAGRetriever:
        """åˆ›å»ºæ£€ç´¢å™¨"""
        if not self.vector_store or len(self.vector_store.documents) == 0:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨build_vector_store()")
        
        self.retriever = RAGRetriever(
            vector_store=self.vector_store,
            reranker=self.reranker,
            embed_top_k=self.config.embed_top_k,
            final_top_k=self.config.final_top_k
        )
        
        print("ğŸ¯ RAGæ£€ç´¢å™¨åˆ›å»ºå®Œæˆ")
        return self.retriever
    
    def search(self, query: str) -> List[Dict]:
        """æ‰§è¡Œæ£€ç´¢"""
        if not self.retriever:
            raise ValueError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨create_retriever()")
        
        docs = self.retriever.get_relevant_documents(query)
        
        results = []
        for doc in docs:
            results.append({
                'rank': doc.metadata.get('final_rank', 0),
                'content': doc.page_content,
                'source': doc.metadata.get('source', 'unknown'),
                'filename': doc.metadata.get('filename', 'unknown'),
                'embed_score': doc.metadata.get('embed_score', 0.0),
                'rerank_score': doc.metadata.get('rerank_score', 0.0),
                'relevance': self._get_relevance_label(doc.metadata.get('rerank_score', 0.0))
            })
        
        return results
    
    def _get_relevance_label(self, score: float) -> str:
        """è·å–ç›¸å…³æ€§æ ‡ç­¾"""
        if score > 0.7:
            return "ğŸ”¥ é«˜åº¦ç›¸å…³"
        elif score > 0.4:
            return "ğŸ“„ ä¸­ç­‰ç›¸å…³"
        else:
            return "â“ ä½ç›¸å…³"


def demo():
    """æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ è½»é‡çº§RAGç³»ç»Ÿæ¼”ç¤ºï¼ˆæ— LangChainä¾èµ–ï¼‰")
    print("="*60)
    
    # é…ç½®å‚æ•°
    config = RAGConfig(
        embed_model_path="/mnt/e/qwenrag/embed",     # ä¿®æ”¹ä¸ºä½ çš„åµŒå…¥æ¨¡å‹è·¯å¾„
        rerank_model_path="/mnt/e/qwenrag/rerank",   # ä¿®æ”¹ä¸ºä½ çš„é‡æ’åºæ¨¡å‹è·¯å¾„
        txt_data_path="/mnt/e/qwenrag/Rag_System/data/processed/LiHua-World",  # æ”¯æŒé€’å½’æŸ¥æ‰¾å­ç›®å½•ä¸­çš„TXTæ–‡ä»¶
        vector_store_path="/mnt/e/qwenrag/Rag_System/data/vectors",
        embed_top_k=6,
        final_top_k=3
    )
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        rag_system = LightweightRAGSystem(config)
        
        # æ„å»ºå‘é‡å­˜å‚¨ï¼ˆåªæœ‰æ•°æ®å˜åŒ–æ—¶æ‰é‡æ–°æ„å»ºï¼‰
        rag_system.build_vector_store(force_rebuild=False)
        
        # åˆ›å»ºæ£€ç´¢å™¨
        rag_system.create_retriever()
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "gym workout plan",
            "RAG system implementation",
            "epic photos"
        ]
        
        for query in test_queries:
            print(f"\n{'='*60}")
            print(f"ğŸ” æŸ¥è¯¢: {query}")
            print(f"{'='*60}")
            
            try:
                results = rag_system.search(query)
                
                if results:
                    print(f"\nğŸ† æ£€ç´¢ç»“æœ (å…±{len(results)}æ¡):")
                    for result in results:
                        print(f"\n{result['rank']}. {result['relevance']}")
                        print(f"   æ–‡ä»¶: {result['filename']}")
                        print(f"   å‘é‡ç›¸ä¼¼åº¦: {result['embed_score']:.3f}")
                        print(f"   é‡æ’åºè¯„åˆ†: {result['rerank_score']:.3f}")
                        print(f"   å†…å®¹é¢„è§ˆ: {result['content'][:200]}...")
                else:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
                    
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
    
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    demo()


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºLangChainçš„RAGæ£€ç´¢é‡æ’åºç³»ç»Ÿ
ä½¿ç”¨LangChainæ¡†æ¶é‡æ„åŸæœ‰ç³»ç»Ÿï¼Œæä¾›æ›´å¥½çš„æ‰©å±•æ€§å’Œç»´æŠ¤æ€§
"""

import os
import json
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
import warnings
import torch
import numpy as np
from pathlib import Path
import hashlib

# LangChainæ ¸å¿ƒç»„ä»¶
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from langchain_core.pydantic_v1 import Field

# LangChainæ–‡æ¡£åŠ è½½å™¨
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# LangChainå‘é‡å­˜å‚¨
from langchain_community.vectorstores import FAISS

# VLLMå’ŒTransformers
try:
    from vllm import LLM, SamplingParams
    from vllm.inputs.data import TokensPrompt
    from transformers import AutoTokenizer
    print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰å¿…éœ€åº“")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·å®‰è£…: pip install langchain langchain-community faiss-cpu vllm transformers torch")
    exit(1)

warnings.filterwarnings("ignore")


@dataclass
class RAGConfig:
    """RAGç³»ç»Ÿé…ç½®ç±»"""
    embed_model_path: str
    rerank_model_path: str
    txt_data_path: str
    vector_store_path: str = "./vector_store"
    embed_top_k: int = 10
    final_top_k: int = 5
    chunk_size: int = 1000
    chunk_overlap: int = 200
    device: str = "cuda" if torch.cuda.is_available() else "cpu"


class QwenEmbeddings(Embeddings):
    """
    ç»§æ‰¿LangChainçš„EmbeddingsåŸºç±»çš„QwenåµŒå…¥æ¨¡å‹
    æä¾›æ ‡å‡†çš„LangChainåµŒå…¥æ¥å£
    """
    
    def __init__(self, model_path: str):
        super().__init__()
        self.model_path = model_path
        print(f"ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹: {model_path}")
        self.model = LLM(model=model_path, task="embed")
        print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£åˆ—è¡¨ - LangChainæ ‡å‡†æ¥å£"""
        if not texts:
            return []
        
        # ä¸ºæ–‡æ¡£æ·»åŠ æŒ‡ä»¤å‰ç¼€
        prefixed_texts = [
            f"Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: {text}"
            for text in texts
        ]
        
        print(f"ğŸ”„ æ­£åœ¨å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æ¡£...")
        outputs = self.model.embed(prefixed_texts)
        embeddings = [output.outputs.embedding for output in outputs]
        print(f"âœ… æ–‡æ¡£å‘é‡åŒ–å®Œæˆ")
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢æ–‡æœ¬ - LangChainæ ‡å‡†æ¥å£"""
        prefixed_text = f"Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: {text}"
        outputs = self.model.embed([prefixed_text])
        return outputs[0].outputs.embedding


class QwenReranker:
    """Qwené‡æ’åºå™¨"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        print(f"ğŸ“¦ åŠ è½½é‡æ’åºæ¨¡å‹: {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = LLM(
            model=model_path,
            max_model_len=8192,
            trust_remote_code=True,
            gpu_memory_utilization=0.8
        )
        self._setup_reranker()
        print("âœ… é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _setup_reranker(self):
        """è®¾ç½®é‡æ’åºå™¨å‚æ•°"""
        self.suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)
        
        self.yes_token = self.tokenizer("yes", add_special_tokens=False).input_ids[0]
        self.no_token = self.tokenizer("no", add_special_tokens=False).input_ids[0]
        
        self.sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=1,
            logprobs=20,
            allowed_token_ids=[self.yes_token, self.no_token]
        )
    
    def _create_rerank_prompt(self, query: str, document: str, instruction: str) -> List[dict]:
        """åˆ›å»ºé‡æ’åºæç¤ºè¯"""
        return [
            {
                "role": "system",
                "content": "Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\"."
            },
            {
                "role": "user",
                "content": f"<Instruct>: {instruction}\n\n<Query>: {query}\n\n<Document>: {document}"
            }
        ]
    
    def score(self, query: str, document: str, 
              instruction: str = "Given a web search query, retrieve relevant passages that answer the query") -> float:
        """è®¡ç®—é‡æ’åºè¯„åˆ†"""
        messages = [self._create_rerank_prompt(query, document, instruction)]
        
        tokenized = self.tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=False
        )[0]
        
        full_tokens = tokenized + self.suffix_tokens
        token_prompt = TokensPrompt(prompt_token_ids=full_tokens)
        
        outputs = self.model.generate([token_prompt], self.sampling_params)
        logprobs = outputs[0].outputs[0].logprobs[-1]
        
        yes_logprob = logprobs.get(self.yes_token, type('obj', (object,), {'logprob': -10})()).logprob
        no_logprob = logprobs.get(self.no_token, type('obj', (object,), {'logprob': -10})()).logprob
        
        yes_prob = np.exp(yes_logprob)
        no_prob = np.exp(no_logprob)
        
        total_prob = yes_prob + no_prob
        score = yes_prob / total_prob if total_prob > 0 else 0.0
        
        return float(score)


class RerankRetriever(BaseRetriever):
    """
    ç»§æ‰¿LangChain BaseRetrieverçš„é‡æ’åºæ£€ç´¢å™¨
    å®ç°æ ‡å‡†çš„LangChainæ£€ç´¢å™¨æ¥å£
    """
    
    vector_store: FAISS = Field(description="å‘é‡å­˜å‚¨")
    reranker: QwenReranker = Field(description="é‡æ’åºå™¨")
    embed_top_k: int = Field(default=10, description="å‘é‡æ£€ç´¢è¿”å›æ•°é‡")
    final_top_k: int = Field(default=5, description="æœ€ç»ˆè¿”å›æ•°é‡")
    
    class Config:
        arbitrary_types_allowed = True
    
    def _get_relevant_documents(
        self, 
        query: str, 
        *, 
        run_manager: CallbackManagerForRetrieverRun = None
    ) -> List[Document]:
        """è·å–ç›¸å…³æ–‡æ¡£ - LangChainæ ‡å‡†æ¥å£"""
        print(f"\nğŸ” å¼€å§‹RAGæ£€ç´¢: '{query}'")
        print("="*60)
        
        # ç¬¬ä¸€æ­¥ï¼šå‘é‡æ£€ç´¢
        print(f"ğŸ“Š æ‰§è¡Œå‘é‡æ£€ç´¢ï¼Œè·å–top-{self.embed_top_k}æ–‡æ¡£...")
        embed_docs = self.vector_store.similarity_search_with_score(query, k=self.embed_top_k)
        
        if not embed_docs:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            return []
        
        print(f"âœ… å‘é‡æ£€ç´¢å®Œæˆï¼Œè·å¾— {len(embed_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
        
        # ç¬¬äºŒæ­¥ï¼šé‡æ’åº
        print(f"ğŸ¯ æ‰§è¡Œé‡æ’åº...")
        rerank_results = []
        
        for i, (doc, embed_score) in enumerate(embed_docs):
            rerank_score = self.reranker.score(query, doc.page_content)
            rerank_results.append((doc, embed_score, rerank_score))
            print(f"  æ–‡æ¡£ {i+1}/{len(embed_docs)}: å‘é‡={embed_score:.3f}, é‡æ’åº={rerank_score:.3f}")
        
        # æŒ‰é‡æ’åºè¯„åˆ†æ’åº
        rerank_results.sort(key=lambda x: x[2], reverse=True)
        
        # è¿”å›æœ€ç»ˆç»“æœ
        final_docs = []
        for i, (doc, embed_score, rerank_score) in enumerate(rerank_results[:self.final_top_k]):
            # æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®
            doc.metadata.update({
                'embed_score': embed_score,
                'rerank_score': rerank_score,
                'final_rank': i + 1
            })
            final_docs.append(doc)
        
        print(f"âœ… é‡æ’åºå®Œæˆï¼Œè¿”å›top-{len(final_docs)}ç»“æœ")
        return final_docs


class LangChainRAGSystem:
    """åŸºäºLangChainçš„RAGç³»ç»Ÿ"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.embeddings = None
        self.reranker = None
        self.vector_store = None
        self.retriever = None
        self.text_splitter = None
        
        self._initialize_components()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        print("ğŸš€ åˆå§‹åŒ–LangChain RAGç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = QwenEmbeddings(self.config.embed_model_path)
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        self.reranker = QwenReranker(self.config.rerank_model_path)
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\n\n", "\n", " ", ""]
        )
        
        print("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def _get_data_hash(self) -> str:
        """è·å–æ•°æ®ç›®å½•çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹æ•°æ®æ˜¯å¦å˜åŒ–"""
        data_path = Path(self.config.txt_data_path)
        
        if data_path.is_file():
            with open(data_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        
        # ç›®å½•æƒ…å†µï¼šé€’å½’è®¡ç®—æ‰€æœ‰txtæ–‡ä»¶çš„ç»¼åˆå“ˆå¸Œ
        hash_md5 = hashlib.md5()
        txt_files = sorted(data_path.rglob("*.txt"))  # é€’å½’æŸ¥æ‰¾å¹¶æ’åºç¡®ä¿ä¸€è‡´æ€§
        
        for txt_file in txt_files:
            try:
                with open(txt_file, 'rb') as f:
                    hash_md5.update(f.read())
                hash_md5.update(str(txt_file).encode())  # åŒ…å«æ–‡ä»¶å
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•è¯»å–æ–‡ä»¶ {txt_file}: {e}")
        
        return hash_md5.hexdigest()
    
    def _check_vector_store_validity(self) -> bool:
        """æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦æœ‰æ•ˆä¸”æœ€æ–°"""
        vector_path = Path(self.config.vector_store_path)
        hash_file = vector_path / "data_hash.txt"
        
        if not hash_file.exists():
            return False
        
        try:
            with open(hash_file, 'r') as f:
                stored_hash = f.read().strip()
            
            current_hash = self._get_data_hash()
            return stored_hash == current_hash
        
        except Exception:
            return False
    
    def load_documents(self) -> List[Document]:
        """ä½¿ç”¨LangChainåŠ è½½æ–‡æ¡£"""
        data_path = Path(self.config.txt_data_path)
        
        if data_path.is_file() and data_path.suffix == '.txt':
            # å•ä¸ªæ–‡ä»¶
            loader = TextLoader(str(data_path), encoding='utf-8')
            docs = loader.load()
        elif data_path.is_dir():
            # ç›®å½•åŠ è½½å™¨ï¼Œæ”¯æŒé€’å½’æŸ¥æ‰¾
            loader = DirectoryLoader(
                str(data_path),
                glob="**/*.txt",  # é€’å½’æŸ¥æ‰¾æ‰€æœ‰txtæ–‡ä»¶
                loader_cls=TextLoader,
                loader_kwargs={'encoding': 'utf-8'},
                show_progress=True
            )
            docs = loader.load()
        else:
            raise ValueError(f"æ— æ•ˆçš„æ•°æ®è·¯å¾„: {data_path}")
        
        print(f"ğŸ“ ä½¿ç”¨LangChainåŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£")
        
        # æ–‡æ¡£åˆ†å‰²ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.config.chunk_size > 0:
            print(f"ğŸ“„ å¼€å§‹æ–‡æ¡£åˆ†å‰²ï¼Œå—å¤§å°: {self.config.chunk_size}, é‡å : {self.config.chunk_overlap}")
            docs = self.text_splitter.split_documents(docs)
            print(f"ğŸ“„ åˆ†å‰²å®Œæˆï¼Œæ€»å…± {len(docs)} ä¸ªæ–‡æ¡£å—")
        
        return docs
    
    def build_vector_store(self, force_rebuild: bool = False) -> FAISS:
        """æ„å»ºæˆ–åŠ è½½å‘é‡å­˜å‚¨"""
        vector_path = Path(self.config.vector_store_path)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        need_rebuild = force_rebuild or not self._check_vector_store_validity()
        
        if not need_rebuild and vector_path.exists():
            print("ğŸ“‚ æ£€æµ‹åˆ°æœ‰æ•ˆçš„å‘é‡å­˜å‚¨ç¼“å­˜...")
            try:
                self.vector_store = FAISS.load_local(
                    str(vector_path), 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("âœ… å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸï¼Œè·³è¿‡é‡æ–°æ„å»º")
                return self.vector_store
            except Exception as e:
                print(f"âš ï¸  å‘é‡å­˜å‚¨åŠ è½½å¤±è´¥: {e}ï¼Œå°†é‡æ–°æ„å»º")
                need_rebuild = True
        
        if need_rebuild:
            print("ğŸ—ï¸  æ„å»ºæ–°çš„å‘é‡å­˜å‚¨...")
            
            # ä½¿ç”¨LangChainåŠ è½½æ–‡æ¡£
            documents = self.load_documents()
            
            if not documents:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£")
            
            # ä½¿ç”¨LangChain FAISSæ„å»ºå‘é‡å­˜å‚¨
            print("ğŸ”¨ å¼€å§‹å‘é‡åŒ–æ–‡æ¡£...")
            self.vector_store = FAISS.from_documents(documents, self.embeddings)
            
            # ä¿å­˜å‘é‡å­˜å‚¨
            vector_path.mkdir(parents=True, exist_ok=True)
            self.vector_store.save_local(str(vector_path))
            
            # ä¿å­˜æ•°æ®å“ˆå¸Œ
            hash_file = vector_path / "data_hash.txt"
            with open(hash_file, 'w') as f:
                f.write(self._get_data_hash())
            
            print("âœ… å‘é‡å­˜å‚¨æ„å»ºå¹¶ä¿å­˜å®Œæˆ")
        
        return self.vector_store
    
    def create_retriever(self) -> RerankRetriever:
        """åˆ›å»ºæ£€ç´¢å™¨"""
        if not self.vector_store:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨build_vector_store()")
        
        self.retriever = RerankRetriever(
            vector_store=self.vector_store,
            reranker=self.reranker,
            embed_top_k=self.config.embed_top_k,
            final_top_k=self.config.final_top_k
        )
        
        print("ğŸ¯ LangChain RAGæ£€ç´¢å™¨åˆ›å»ºå®Œæˆ")
        return self.retriever
    
    def search(self, query: str) -> List[Dict]:
        """æ‰§è¡Œæ£€ç´¢"""
        if not self.retriever:
            raise ValueError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨create_retriever()")
        
        # ä½¿ç”¨LangChainæ ‡å‡†æ¥å£
        docs = self.retriever.get_relevant_documents(query)
        
        results = []
        for doc in docs:
            results.append({
                'rank': doc.metadata.get('final_rank', 0),
                'content': doc.page_content,
                'source': doc.metadata.get('source', 'unknown'),
                'embed_score': doc.metadata.get('embed_score', 0.0),
                'rerank_score': doc.metadata.get('rerank_score', 0.0),
                'relevance': self._get_relevance_label(doc.metadata.get('rerank_score', 0.0))
            })
        
        return results
    
    def _get_relevance_label(self, score: float) -> str:
        """è·å–ç›¸å…³æ€§æ ‡ç­¾"""
        if score > 0.7:
            return "ğŸ”¥ é«˜åº¦ç›¸å…³"
        elif score > 0.4:
            return "ğŸ“„ ä¸­ç­‰ç›¸å…³"
        else:
            return "â“ ä½ç›¸å…³"
    
    def get_retriever_as_langchain_tool(self):
        """è·å–å¯ç”¨äºLangChain Agentçš„æ£€ç´¢å·¥å…·"""
        from langchain.tools.retriever import create_retriever_tool
        
        retriever_tool = create_retriever_tool(
            self.retriever,
            "rag_retriever",
            "è¿™æ˜¯ä¸€ä¸ªRAGæ£€ç´¢å·¥å…·ï¼Œå¯ä»¥æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚ç”¨äºå›ç­”éœ€è¦æŸ¥æ‰¾çŸ¥è¯†åº“ä¿¡æ¯çš„é—®é¢˜ã€‚"
        )
        
        return retriever_tool


def demo():
    """æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ LangChain RAGç³»ç»Ÿæ¼”ç¤º")
    print("="*60)
    
    # é…ç½®å‚æ•°
    config = RAGConfig(
        embed_model_path="/mnt/e/qwenrag/embed",     # ä¿®æ”¹ä¸ºä½ çš„åµŒå…¥æ¨¡å‹è·¯å¾„
        rerank_model_path="/mnt/e/qwenrag/rerank",   # ä¿®æ”¹ä¸ºä½ çš„é‡æ’åºæ¨¡å‹è·¯å¾„
        txt_data_path="/mnt/e/qwenrag/Rag_System/data/processed/LiHua-World",  # æ”¯æŒé€’å½’æŸ¥æ‰¾å­ç›®å½•ä¸­çš„TXTæ–‡ä»¶
        vector_store_path="/mnt/e/qwenrag/Rag_System/data/vectors_langchain",
        embed_top_k=6,
        final_top_k=3,
        chunk_size=1000,  # å¯ç”¨æ–‡æ¡£åˆ†å‰²
        chunk_overlap=200
    )
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        rag_system = LangChainRAGSystem(config)
        
        # æ„å»ºå‘é‡å­˜å‚¨ï¼ˆåªæœ‰æ•°æ®å˜åŒ–æ—¶æ‰é‡æ–°æ„å»ºï¼‰
        rag_system.build_vector_store(force_rebuild=False)
        
        # åˆ›å»ºæ£€ç´¢å™¨
        rag_system.create_retriever()
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "gym workout plan",
            "RAG system implementation",
            "epic photos"
        ]
        
        for query in test_queries:
            print(f"\n{'='*60}")
            print(f"ğŸ” æŸ¥è¯¢: {query}")
            print(f"{'='*60}")
            
            try:
                results = rag_system.search(query)
                
                if results:
                    print(f"\nğŸ† æ£€ç´¢ç»“æœ (å…±{len(results)}æ¡):")
                    for result in results:
                        print(f"\n{result['rank']}. {result['relevance']}")
                        print(f"   æ¥æº: {Path(result['source']).name}")
                        print(f"   å‘é‡ç›¸ä¼¼åº¦: {result['embed_score']:.3f}")
                        print(f"   é‡æ’åºè¯„åˆ†: {result['rerank_score']:.3f}")
                        print(f"   å†…å®¹é¢„è§ˆ: {result['content'][:200]}...")
                else:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
                    
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
        
        # æ¼”ç¤ºè·å–LangChainå·¥å…·
        print(f"\n{'='*60}")
        print("ğŸ› ï¸  è·å–LangChainæ£€ç´¢å·¥å…·")
        print(f"{'='*60}")
        retriever_tool = rag_system.get_retriever_as_langchain_tool()
        print(f"âœ… å·¥å…·åˆ›å»ºå®Œæˆ: {retriever_tool.name}")
        print(f"   æè¿°: {retriever_tool.description}")
    
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    demo()